<!DOCTYPE html>
<html>
<head>
<link rel="stylesheet" href="index.css">
</head>
    <body>

    <div id='navBar'>
        <ul id='leftNav'>
            <li><a href="index.html">TCAB</a></li>
        </ul>

        <ul id='rightNav'>
            <li class='active'><a href="index.html">Home</a></li>
            <li><a href="datasheet.html">Datasheet</a></li>
            <li><a href="https://zenodo.org/record/7226519" target="_blank" rel="noopener noreferrer">Download</a></li>
            <li><a href="https://github.com/REACT-NLP" target="_blank" rel="noopener noreferrer">Github</a></li>
        </ul>
    </div>

    <div id='splashDiv'>
        <h1 id='splashTitle'>TCAB</h1>
        <h7 id='splashSubtitle'>The Text Classification Attack Benchmark</h7>
    </div>

    <div id='mainDiv'>
        <h3 class='subsectionHeader'>What is TCAB?</h3><hr>
        <p>
            TCAB is a set of benchmark datasets for analyzing, understanding, detecting, and labeling adversarial attacks against text classifiers. TCAB includes over 1.5 million successful attack instances, generated by twelve adversarial attacks targeting three classifiers trained on six source datasets for sentiment analysis and
            abuse detection in English.
        </p>
        <br>
        <h3 class='subsectionHeader'>Why was TCAB created?</h3><hr>
        <p>
            A common defense strategy against adversarial attacks is to make classifiers more robust, thus most evaluation frameworks focus on model robustness. However, these defenses are often computationally expensive or result in reduced accuracy. Additionally, benchmarks that evaluate these defenses require carefully controlled and expensive human filtering, typically resulting in a small number of evaluation examples.
        </p>
        <p>
            TCAB comprises a large collection of fully-automated attacks, enabling new tasks such as attack labeling --- automatically determining the adversarial attacks (if any) used to generate a given piece of text --- attack localization, and attack characterization. As a complement to model robustness, TCAB facilitates research that enables one to learn more about their attackers and subsequently develop appropriate defenses.
        </p>
        <br>
        <h3 class='subsectionHeader'>How was TCAB created?</h3><hr>
        <p>
            After training a target model on a particular domain dataset, TCAB generates adversarial examples by attacking instances in the test set using <a href='https://github.com/QData/TextAttack' target="_blank" rel="noopener noreferrer">TextAttack</a> or <a href='https://github.com/thunlp/OpenAttack' target="_blank" rel="noopener noreferrer">OpenAttack</a> --- two open source toolchains that provide fully-automated off-the-shelf attacks.
        </p>
        <p>
            TCAB contains attacks from methods that cover a wide range of design choices and assumptions, such as model access level (e.g., white/gray/black box), perturbation level (e.g., char/word/token), and linguistic constraints that can help make attacks more indistinguishable from the original text.
        </p>
        <br>
        <h3 class='subsectionHeader'>Human Evaluation</h3><hr>
        <p>
            We adopt crowdsourcing to label a portion of the adversarial examples in TCAB to get a sense of how often perturbed instances preserve their original labels. In total, 5,581 adversarial examples were annoted by workers from <a href="https://www.mturk.com/">Amazon Mechanical Turk</a>, with each instance labeled by 5 different workers. We observe that on average, 51% and 81% of the adversarial instances' labels were preserved for sentiment analysis and abuse detection datasets, respectively.
        </p>
        <br>
        <h3 class='subsectionHeader'>Getting Started</h3><hr>
        <p>
            Download a copy of the <a href="https://zenodo.org/record/7226519" target="_blank" rel="noopener noreferrer">dataset</a> hosted on Zenodo (distributed with a <a href="https://creativecommons.org/licenses/by/4.0/" target="_blank" rel="noopener noreferrer">CC-BY 4.0</a> license). The dataset contains two files that include a training set and a validation set.
        </p>
        <br>
        <h3 class='subsectionHeader'>Baseline Models</h3><hr>
        <p>
            We provide a baseline approach for attack detection and labeling that combines contextualized embeddings from a fine-tuned BERT model and hand-crafted text, language model, and target model properties. Our baseline approach achieves 91.7% and 66.7% accuracy for attack detection and labeling, on average. Code for these baselines is in the <a href="https://github.com/REACT-NLP/tcab_benchmark">TCAB Benchmark</a> repository on Github.
        </p>
        <br>
        <h3 class='subsectionHeader'>Extending TCAB</h3><hr>
        <p>
            TCAB is designed to be extended with additional datasets as new attack methods and text classifiers are developed. To add a new domain dataset or attack, follow the instructions in the <a href="https://github.com/REACT-NLP/tcab_generation" target="_blank" rel="noopener noreferrer">TCAB Generation</a> repository on GitHub.
        </p>
        <br>
        <h3 class='subsectionHeader'>Who Created TCAB?</h3><hr>
        <p>
            TCAB was created by researchers at the Univesity of Oregon and the University of California Irvine. Please direct questions to <a href="mailto: lowd@cs.uoregon.edu">lowd@cs.uoregon.edu</a> or <a href="mailto: sameer@uci.edu">sameer@uci.edu</a>.
        </p>
    </div>

    <script type="text/javascript" src='index.js'></script>
</body>
</html>
